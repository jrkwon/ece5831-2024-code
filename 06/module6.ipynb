{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_hat, y):\n",
    "    return np.sum((y_hat - y)**2)/y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.025000000000000022"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y      = np.array([1,   2,   3,    4])\n",
    "y_hat1 = np.array([1.2, 1.9, 2.9,  4.2]) \n",
    "mean_squared_error(y_hat1, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0250000000000004"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat2 = np.array([2.2, 0.9, 2.9,  5.2]) \n",
    "mean_squared_error(y_hat2, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y_hat, y):\n",
    "    return -np.sum(y*np.log(y_hat + 1e-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([0, 1, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat1 = np.array([0.1, 0.7, 0.1, 0.1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3566748010815999"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(y_hat1, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat2 = np.array([0.7, 0.05, 0.05, 0.2, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9957302735559908"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(y_hat2, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz...\n",
      "Done\n",
      "Downloading train-labels-idx1-ubyte.gz...\n",
      "Done\n",
      "Downloading t10k-images-idx3-ubyte.gz...\n",
      "Done\n",
      "Downloading t10k-labels-idx1-ubyte.gz...\n",
      "Done\n",
      "Pickle: dataset/mnist.pkl is being created.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import mnist\n",
    "\n",
    "my_mnist = mnist.Mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (_, _) = my_mnist.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = train_images.shape[0]\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50427 40534 29906 41290  8956 25127 45645 50648 35388 46412  1825 50880\n",
      " 43775 37044 23823 21228 47835 23341 47817 29253 42366 38032 24516   499\n",
      " 17938 40750 43103 46096 17899 49851  8680 25232]\n"
     ]
    }
   ],
   "source": [
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "print(batch_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for mini-batch training.\n",
    "def cross_entropy_error(y_hat, y):\n",
    "    batch_size = 1 if y_hat.ndim == 1 else y_hat.shape[0]\n",
    "    return -np.sum(y*np.log(y_hat + 1e-7))/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_batch = np.array([ [0.2, 0.2, 0.3, 0.1, 0.2], [0.1, 0.1, 0.1, 0.1, 0.6]])\n",
    "y_batch =     np.array([ [0,   0,   1,   0,    0],   [0,   0,   0,   0,   1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8573989640459981"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(y_hat_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e+48"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.1/10e-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 10e-50\n",
    "    return (f(x + h) - f(x))/h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    return x**2 + 0.1*x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(func, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more reasonable approximation\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x + h) - f(x - h))/(2*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6999999999994797"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(func, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49999999999994493"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(func, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "def func_tmp1(x0):\n",
    "    return x0**2 + 4.0**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(func_tmp1, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_tmp2(x1):\n",
    "    return 3.0**2 + x1**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(func_tmp2, 4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial derivatives when x0 = 3, x1 = 4\n",
    "\n",
    "def func_tmp1(x0):\n",
    "    return x0**2 + 4**2\n",
    "\n",
    "def func_tmp2(x1):\n",
    "    return 3**2 + x1**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x + h) - f(x - h))/(2*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x) \n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        # f(x+h) \n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h) \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) \n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val \n",
    "        \n",
    "    return grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_numerical_diff(func_tmp1, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_numerical_diff(func_tmp2, 4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_numerical_gradient(func2, np.array([3.0, 4.0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.1, step_num = 100):\n",
    "    x = init_x\n",
    "    for i in range(step_num):\n",
    "        grad = _numerical_gradient(f, x)\n",
    "        x -= lr*grad  # x = x - lr*grad\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.65680105e-06, 2.02028609e-06])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x = np.array([2800.0, 1000.0])\n",
    "# func2 = x0**2 + x1**2\n",
    "gradient_descent(func2, init_x, step_num=10000, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet:\n",
    "    def __init__(self):\n",
    "        self.w = np.random.randn(2, 3)\n",
    "\n",
    "\n",
    "    # for multi-dimensional x\n",
    "    def softmax(self, x):\n",
    "        if x.ndim == 2:\n",
    "            x = x.T\n",
    "            x = x - np.max(x, axis=0)\n",
    "            y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "            return y.T \n",
    "\n",
    "        x = x - np.max(x)  \n",
    "        return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "    def cross_entroy_error(self, y, t):\n",
    "        delta = 1e-7\n",
    "        batch_size = 1 if y.ndim == 1 else y.shape[0]\n",
    "\n",
    "        return -np.sum(t*np.log(y + delta)) / batch_size\n",
    "\n",
    "\n",
    "    # for multi-dimensional x\n",
    "    def numerical_gradient(self, f, x):\n",
    "        h = 1e-4 # 0.0001\n",
    "        grad = np.zeros_like(x)\n",
    "        \n",
    "        it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            idx = it.multi_index\n",
    "            tmp_val = x[idx]\n",
    "            x[idx] = float(tmp_val) + h\n",
    "            fxh1 = f(x) # f(x+h)\n",
    "            \n",
    "            x[idx] = tmp_val - h \n",
    "            fxh2 = f(x) # f(x-h)\n",
    "            grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "            \n",
    "            x[idx] = tmp_val \n",
    "            it.iternext()   \n",
    "            \n",
    "        return grad\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.w)\n",
    "    \n",
    "\n",
    "    def loss(self, x, y):\n",
    "        z = self.predict(x)\n",
    "        y_hat = self.softmax(z)\n",
    "        loss = self.cross_entroy_error(y_hat, y)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.68927577 -1.11643295 -0.13601282]\n",
      " [ 0.60219774  0.58673274 -1.60266364]]\n"
     ]
    }
   ],
   "source": [
    "# let's test SimpleNet\n",
    "net = SimpleNet()\n",
    "print(net.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.16565823e-01  6.67160157e-01  3.27396510e-04]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.7, 0.19])\n",
    "p = net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6780501115099065"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([0, 1, 0])\n",
    "net.loss(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3448826849923987"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([0, 0, 1])\n",
    "net.loss(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(w):\n",
    "    return net.loss(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.42403234  0.1194473  -0.54347965]\n",
      " [ 0.11509449  0.03242141 -0.1475159 ]]\n"
     ]
    }
   ],
   "source": [
    "dw = net.numerical_gradient(loss_function, net.w)\n",
    "print(dw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using lamda\n",
    "loss_function = lambda w: net.loss(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.42403234  0.1194473  -0.54347965]\n",
      " [ 0.11509449  0.03242141 -0.1475159 ]]\n"
     ]
    }
   ],
   "source": [
    "dw = net.numerical_gradient(loss_function, net.w)\n",
    "print(dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TwoLayerNet class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activations:\n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    \n",
    "    # for multi-dimensional x\n",
    "    def softmax(self, x):\n",
    "        if x.ndim == 2:\n",
    "            x = x.T\n",
    "            x = x - np.max(x, axis=0)\n",
    "            y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "            return y.T \n",
    "\n",
    "        x = x - np.max(x)  \n",
    "        return np.exp(x) / np.sum(np.exp(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Errors:\n",
    "    def cross_entroy_error(self, y, t):\n",
    "        delta = 1e-7\n",
    "        batch_size = 1 if y.ndim == 1 else y.shape[0]\n",
    "\n",
    "        return -np.sum(t*np.log(y + delta)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import activations\n",
    "import errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "\n",
    "        self.params['w1'] = weight_init_std*np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "\n",
    "        self.params['w2'] = weight_init_std*np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        self.activations = activations.Activations()\n",
    "        self.errors = errors.Errors()\n",
    "\n",
    "    def predict(self, x):\n",
    "        w1, w2 = self.params['w1'], self.params['w2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, w1) + b1\n",
    "        z1 = self.activations.sigmoid(a1)\n",
    "        a2 = np.dot(z1, w2) + b2\n",
    "        y = self.activations.softmax(a2)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        y_hat = self.predict(x)\n",
    "\n",
    "        return self.errors.cross_entropy_error(y_hat, y)\n",
    "    \n",
    "\n",
    "    def accuracy(self, x, y):\n",
    "        y_hat = self.predict(x)\n",
    "        p = np.argmax(y_hat, axis=1)\n",
    "        y_p = np.argmax(y, axis=1)\n",
    "\n",
    "        return np.sum(p == y_p)/float(x.shape[0])\n",
    "    \n",
    "\n",
    "    # for multi-dimensional x\n",
    "    def _numerical_gradient(self, f, x):\n",
    "        h = 1e-4 # 0.0001\n",
    "        grad = np.zeros_like(x)\n",
    "        \n",
    "        it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            idx = it.multi_index\n",
    "            tmp_val = x[idx]\n",
    "            x[idx] = float(tmp_val) + h\n",
    "            fxh1 = f(x) # f(x+h)\n",
    "            \n",
    "            x[idx] = tmp_val - h \n",
    "            fxh2 = f(x) # f(x-h)\n",
    "            grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "            \n",
    "            x[idx] = tmp_val \n",
    "            it.iternext()   \n",
    "            \n",
    "        return grad\n",
    "    \n",
    "\n",
    "    def numerical_gradient(self, x, y):\n",
    "        loss_w = lambda w: self.loss(x, y)\n",
    "\n",
    "        grads = {}\n",
    "        grads['w1'] = self._numerical_gradient(loss_w, self.params['w1'])\n",
    "        grads['b1'] = self._numerical_gradient(loss_w, self.params['b1'])\n",
    "        grads['w2'] = self._numerical_gradient(loss_w, self.params['w2'])\n",
    "        grads['b2'] = self._numerical_gradient(loss_w, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train TwoLayerNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist\n",
    "import two_layer_net\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: train-images-idx3-ubyte.gz already exists.\n",
      "File: train-labels-idx1-ubyte.gz already exists.\n",
      "File: t10k-images-idx3-ubyte.gz already exists.\n",
      "File: t10k-labels-idx1-ubyte.gz already exists.\n",
      "Pickle: dataset/mnist.pkl already exists.\n",
      "Loading...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "my_mnist = mnist.Mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = my_mnist.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "network = two_layer_net.TwoLayerNet(input_size=28*28, hidden_size=100, output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 100\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 32\n",
    "lr = 0.1\n",
    "\n",
    "iter_per_ecoph = max(train_size/batch_size, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_accs = []\n",
    "test_accs = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test_acc : 0.10441666666666667, 0.1028\n"
     ]
    }
   ],
   "source": [
    "for i in range(iterations):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "\n",
    "    grads = network.numerical_gradient(x_batch, y_batch)\n",
    "\n",
    "    for key in ('w1', 'b1', 'w2', 'b2'):\n",
    "        network.params[key] -= lr*grads[key]\n",
    "\n",
    "    ## this is for plotting losses over time\n",
    "    train_losses.append(network.loss(x_batch, y_batch))\n",
    "\n",
    "    if i%iter_per_ecoph == 0:\n",
    "        train_acc = network.accuracy(x_train, y_train)\n",
    "        train_accs.append(train_acc)\n",
    "        test_acc = network.accuracy(x_test, y_test)\n",
    "        test_accs.append(test_acc)\n",
    "        print(f'train acc, test_acc : {train_acc}, {test_acc}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_accs))\n",
    "plt.plot(x, train_accs, label='train acc')\n",
    "plt.plot(x, test_accs, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece5831-2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
