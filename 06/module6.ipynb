{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_hat, y):\n",
    "    return np.sum((y_hat - y)**2)/y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.025000000000000022"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y      = np.array([1,   2,   3,    4])\n",
    "y_hat1 = np.array([1.2, 1.9, 2.9,  4.2]) \n",
    "mean_squared_error(y_hat1, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0250000000000004"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat2 = np.array([2.2, 0.9, 2.9,  5.2]) \n",
    "mean_squared_error(y_hat2, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y_hat, y):\n",
    "    return -np.sum(y*np.log(y_hat + 1e-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([0, 1, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat1 = np.array([0.1, 0.7, 0.1, 0.1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3566748010815999"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(y_hat1, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat2 = np.array([0.7, 0.05, 0.05, 0.2, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9957302735559908"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(y_hat2, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz...\n",
      "Done\n",
      "Downloading train-labels-idx1-ubyte.gz...\n",
      "Done\n",
      "Downloading t10k-images-idx3-ubyte.gz...\n",
      "Done\n",
      "Downloading t10k-labels-idx1-ubyte.gz...\n",
      "Done\n",
      "Pickle: dataset/mnist.pkl is being created.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import mnist\n",
    "\n",
    "my_mnist = mnist.Mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (_, _) = my_mnist.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = train_images.shape[0]\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50427 40534 29906 41290  8956 25127 45645 50648 35388 46412  1825 50880\n",
      " 43775 37044 23823 21228 47835 23341 47817 29253 42366 38032 24516   499\n",
      " 17938 40750 43103 46096 17899 49851  8680 25232]\n"
     ]
    }
   ],
   "source": [
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "print(batch_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for mini-batch training.\n",
    "def cross_entropy_error(y_hat, y):\n",
    "    batch_size = 1 if y_hat.ndim == 1 else y_hat.shape[0]\n",
    "    return -np.sum(y*np.log(y_hat + 1e-7))/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_batch = np.array([ [0.2, 0.2, 0.3, 0.1, 0.2], [0.1, 0.1, 0.1, 0.1, 0.6]])\n",
    "y_batch =     np.array([ [0,   0,   1,   0,    0],   [0,   0,   0,   0,   1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8573989640459981"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(y_hat_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e+48"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.1/10e-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 10e-50\n",
    "    return (f(x + h) - f(x))/h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    return x**2 + 0.1*x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(func, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more reasonable approximation\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x + h) - f(x - h))/(2*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6999999999994797"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(func, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49999999999994493"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(func, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "def func_tmp1(x0):\n",
    "    return x0**2 + 4.0**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(func_tmp1, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_tmp2(x1):\n",
    "    return 3.0**2 + x1**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(func_tmp2, 4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial derivatives when x0 = 3, x1 = 4\n",
    "\n",
    "def func_tmp1(x0):\n",
    "    return x0**2 + 4**2\n",
    "\n",
    "def func_tmp2(x1):\n",
    "    return 3**2 + x1**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x + h) - f(x - h))/(2*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x) \n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        # f(x+h) \n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h) \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) \n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val \n",
    "        \n",
    "    return grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_numerical_diff(func_tmp1, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_numerical_diff(func_tmp2, 4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_numerical_gradient(func2, np.array([3.0, 4.0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.1, step_num = 100):\n",
    "    x = init_x\n",
    "    for i in range(step_num):\n",
    "        grad = _numerical_gradient(f, x)\n",
    "        x -= lr*grad  # x = x - lr*grad\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.65680105e-06, 2.02028609e-06])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x = np.array([2800.0, 1000.0])\n",
    "# func2 = x0**2 + x1**2\n",
    "gradient_descent(func2, init_x, step_num=10000, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet:\n",
    "    def __init__(self):\n",
    "        self.w = np.random.randn(2, 3)\n",
    "\n",
    "\n",
    "    # for multi-dimensional x\n",
    "    def softmax(self, x):\n",
    "        if x.ndim == 2:\n",
    "            x = x.T\n",
    "            x = x - np.max(x, axis=0)\n",
    "            y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "            return y.T \n",
    "\n",
    "        x = x - np.max(x)  \n",
    "        return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "    def cross_entroy_error(self, y, t):\n",
    "        delta = 1e-7\n",
    "        batch_size = 1 if y.ndim == 1 else y.shape[0]\n",
    "\n",
    "        return -np.sum(t*np.log(y + delta)) / batch_size\n",
    "\n",
    "\n",
    "    # for multi-dimensional x\n",
    "    def numerical_gradient(self, f, x):\n",
    "        h = 1e-4 # 0.0001\n",
    "        grad = np.zeros_like(x)\n",
    "        \n",
    "        it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            idx = it.multi_index\n",
    "            tmp_val = x[idx]\n",
    "            x[idx] = float(tmp_val) + h\n",
    "            fxh1 = f(x) # f(x+h)\n",
    "            \n",
    "            x[idx] = tmp_val - h \n",
    "            fxh2 = f(x) # f(x-h)\n",
    "            grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "            \n",
    "            x[idx] = tmp_val \n",
    "            it.iternext()   \n",
    "            \n",
    "        return grad\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.w)\n",
    "    \n",
    "\n",
    "    def loss(self, x, y):\n",
    "        z = self.predict(x)\n",
    "        y_hat = self.softmax(z)\n",
    "        loss = self.cross_entroy_error(y_hat, y)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.68927577 -1.11643295 -0.13601282]\n",
      " [ 0.60219774  0.58673274 -1.60266364]]\n"
     ]
    }
   ],
   "source": [
    "# let's test SimpleNet\n",
    "net = SimpleNet()\n",
    "print(net.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.16565823e-01  6.67160157e-01  3.27396510e-04]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.7, 0.19])\n",
    "p = net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6780501115099065"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([0, 1, 0])\n",
    "net.loss(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3448826849923987"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([0, 0, 1])\n",
    "net.loss(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(w):\n",
    "    return net.loss(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.42403234  0.1194473  -0.54347965]\n",
      " [ 0.11509449  0.03242141 -0.1475159 ]]\n"
     ]
    }
   ],
   "source": [
    "dw = net.numerical_gradient(loss_function, net.w)\n",
    "print(dw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using lamda\n",
    "loss_function = lambda w: net.loss(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.42403234  0.1194473  -0.54347965]\n",
      " [ 0.11509449  0.03242141 -0.1475159 ]]\n"
     ]
    }
   ],
   "source": [
    "dw = net.numerical_gradient(loss_function, net.w)\n",
    "print(dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TwoLayerNet class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activations:\n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    \n",
    "    # for multi-dimensional x\n",
    "    def softmax(self, x):\n",
    "        if x.ndim == 2:\n",
    "            x = x.T\n",
    "            x = x - np.max(x, axis=0)\n",
    "            y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "            return y.T \n",
    "\n",
    "        x = x - np.max(x)  \n",
    "        return np.exp(x) / np.sum(np.exp(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Errors:\n",
    "    def cross_entroy_error(self, y, t):\n",
    "        delta = 1e-7\n",
    "        batch_size = 1 if y.ndim == 1 else y.shape[0]\n",
    "\n",
    "        return -np.sum(t*np.log(y + delta)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import activations\n",
    "import errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "\n",
    "        self.params['w1'] = weight_init_std*np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "\n",
    "        self.params['w2'] = weight_init_std*np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        self.activations = activations.Activations()\n",
    "        self.errors = errors.Errors()\n",
    "\n",
    "    def predict(self, x):\n",
    "        w1, w2 = self.params['w1'], self.params['w2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, w1) + b1\n",
    "        z1 = self.activations.sigmoid(a1)\n",
    "        a2 = np.dot(z1, w2) + b2\n",
    "        y = self.activations.softmax(a2)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        y_hat = self.predict(x)\n",
    "\n",
    "        return self.errors.cross_entropy_error(y_hat, y)\n",
    "    \n",
    "\n",
    "    def accuracy(self, x, y):\n",
    "        y_hat = self.predict(x)\n",
    "        p = np.argmax(y_hat, axis=1)\n",
    "        y_p = np.argmax(y, axis=1)\n",
    "\n",
    "        return np.sum(p == y_p)/float(x.shape[0])\n",
    "    \n",
    "\n",
    "    # for multi-dimensional x\n",
    "    def _numerical_gradient(self, f, x):\n",
    "        h = 1e-4 # 0.0001\n",
    "        grad = np.zeros_like(x)\n",
    "        \n",
    "        it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            idx = it.multi_index\n",
    "            tmp_val = x[idx]\n",
    "            x[idx] = float(tmp_val) + h\n",
    "            fxh1 = f(x) # f(x+h)\n",
    "            \n",
    "            x[idx] = tmp_val - h \n",
    "            fxh2 = f(x) # f(x-h)\n",
    "            grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "            \n",
    "            x[idx] = tmp_val \n",
    "            it.iternext()   \n",
    "            \n",
    "        return grad\n",
    "    \n",
    "\n",
    "    def numerical_gradient(self, x, y):\n",
    "        loss_w = lambda w: self.loss(x, y)\n",
    "\n",
    "        grads = {}\n",
    "        grads['w1'] = self._numerical_gradient(loss_w, self.params['w1'])\n",
    "        grads['b1'] = self._numerical_gradient(loss_w, self.params['b1'])\n",
    "        grads['w2'] = self._numerical_gradient(loss_w, self.params['w2'])\n",
    "        grads['b2'] = self._numerical_gradient(loss_w, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train TwoLayerNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: train-images-idx3-ubyte.gz already exists.\n",
      "File: train-labels-idx1-ubyte.gz already exists.\n",
      "File: t10k-images-idx3-ubyte.gz already exists.\n",
      "File: t10k-labels-idx1-ubyte.gz already exists.\n",
      "Pickle: dataset/mnist.pkl already exists.\n",
      "Loading...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "my_mnist = mnist.Mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = my_mnist.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "network = TwoLayerNet(input_size=28*28, hidden_size=100, output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w1': array([[ 0.00691821, -0.00766628,  0.0011048 , ..., -0.01444509,\n",
       "         -0.02096234, -0.00539125],\n",
       "        [ 0.02158764, -0.00461593, -0.01653164, ..., -0.01917657,\n",
       "          0.00191896, -0.00962898],\n",
       "        [ 0.00196323, -0.00159347, -0.02317298, ...,  0.00037187,\n",
       "         -0.00677226, -0.00200744],\n",
       "        ...,\n",
       "        [-0.00456412, -0.01142783,  0.00240188, ...,  0.01069273,\n",
       "         -0.01019897, -0.00378351],\n",
       "        [ 0.00144244,  0.01231268,  0.00395283, ..., -0.0180285 ,\n",
       "         -0.00639699, -0.00790048],\n",
       "        [-0.01467352, -0.00049144, -0.01761132, ...,  0.00548518,\n",
       "          0.01314084,  0.00341427]]),\n",
       " 'b1': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'w2': array([[-8.29861325e-03,  1.30006217e-02, -7.58354272e-03,\n",
       "          4.33822642e-05,  5.04783190e-03, -1.54405148e-02,\n",
       "         -1.14032437e-02, -2.31073862e-03, -5.26458929e-03,\n",
       "         -9.61259532e-03],\n",
       "        [ 2.24037718e-02, -2.98240150e-03,  6.79530231e-03,\n",
       "          1.69841104e-03, -1.28829723e-03, -5.94962673e-03,\n",
       "         -2.26922673e-04,  1.15593230e-03, -1.49984809e-02,\n",
       "         -9.27642412e-04],\n",
       "        [-2.07428860e-02, -7.50477921e-03, -2.63611881e-02,\n",
       "         -2.50577191e-03,  5.31995868e-03,  1.10166974e-02,\n",
       "          6.26901106e-03, -8.52061443e-03,  1.54796959e-02,\n",
       "          4.98446088e-04],\n",
       "        [ 1.38282105e-02, -3.64997435e-03,  1.26178580e-02,\n",
       "          6.29914931e-03, -1.57184637e-02,  9.57097081e-03,\n",
       "          7.85605032e-03, -1.00689260e-02, -1.17276721e-02,\n",
       "         -1.42364272e-02],\n",
       "        [-7.62278008e-03, -1.17477499e-03,  1.82859567e-02,\n",
       "         -3.43898643e-03, -7.88718355e-03, -8.60816546e-03,\n",
       "         -1.87215528e-02, -8.67961730e-04, -1.38204043e-02,\n",
       "         -6.57147112e-04],\n",
       "        [-2.71733977e-03, -7.06802794e-03,  2.00013571e-03,\n",
       "          2.17013970e-02, -1.39873891e-02, -1.72888477e-02,\n",
       "          1.17702587e-02,  2.80656203e-03, -1.67570569e-02,\n",
       "          1.86552193e-02],\n",
       "        [-5.59816876e-03, -3.75255309e-03, -8.04856925e-03,\n",
       "         -7.95341442e-03,  3.77791458e-03, -9.19222905e-04,\n",
       "          1.17036924e-02,  2.21255377e-02, -1.32633263e-02,\n",
       "          8.73391065e-04],\n",
       "        [ 3.90915524e-03,  7.79617651e-04, -1.30910089e-02,\n",
       "         -3.65860982e-03, -2.29589570e-02,  9.11469042e-03,\n",
       "         -3.45370387e-03, -9.37620845e-03, -5.20340221e-03,\n",
       "         -2.18163997e-03],\n",
       "        [ 1.25070821e-02, -1.07086398e-02, -1.03897773e-02,\n",
       "         -8.51013503e-03,  5.67280681e-03,  1.07822542e-04,\n",
       "          5.09744711e-03,  1.69837243e-02,  8.11415905e-03,\n",
       "          1.14025659e-02],\n",
       "        [ 2.15669992e-03, -1.15000779e-03,  1.00490477e-02,\n",
       "          1.32278768e-02,  9.22858470e-04, -8.67481354e-03,\n",
       "          1.63252796e-02, -3.14196652e-03,  1.66585306e-02,\n",
       "          2.37592908e-03],\n",
       "        [-2.92767763e-03,  1.08197088e-02, -2.91472957e-03,\n",
       "         -2.04533873e-02,  4.02410664e-04, -7.99978372e-03,\n",
       "         -3.24705836e-02, -1.20214279e-02,  8.20202352e-03,\n",
       "         -8.12386613e-03],\n",
       "        [ 2.54079953e-03, -4.08047981e-03, -2.80340445e-03,\n",
       "          5.60370967e-03,  3.79908011e-03,  1.53908795e-02,\n",
       "          8.73793521e-03,  3.78629396e-03,  1.91247364e-03,\n",
       "          3.09597672e-03],\n",
       "        [-1.30152939e-02,  1.20072693e-04, -1.53527289e-02,\n",
       "         -2.94921096e-03,  6.16761335e-03, -5.57336338e-03,\n",
       "         -1.80787314e-03,  1.31553973e-02, -1.16247780e-03,\n",
       "          1.53379317e-02],\n",
       "        [-5.50148227e-03,  1.04410571e-02, -2.13532155e-02,\n",
       "         -8.74903300e-03,  6.16721779e-03, -2.62890156e-03,\n",
       "         -3.22618437e-04,  4.18816567e-04,  5.66823912e-03,\n",
       "          1.50536872e-02],\n",
       "        [ 9.37879104e-03,  2.12155147e-02, -1.64375374e-02,\n",
       "         -3.65462444e-03,  1.83770222e-02,  9.25493599e-04,\n",
       "          9.68487922e-03,  4.79910218e-03,  9.49236548e-03,\n",
       "         -1.53545261e-04],\n",
       "        [ 7.16031191e-03,  1.03231002e-02, -3.18896705e-03,\n",
       "         -2.98442726e-03,  5.37128235e-03,  3.00454914e-03,\n",
       "         -2.90186678e-02, -8.60797875e-04,  4.33775760e-03,\n",
       "         -2.48421804e-03],\n",
       "        [-8.94388494e-03,  1.15532419e-02, -6.50962088e-03,\n",
       "          4.11082899e-03,  8.37736292e-03,  1.41071222e-02,\n",
       "         -1.04148490e-03, -1.48605333e-03, -2.12511659e-04,\n",
       "          1.11405617e-02],\n",
       "        [ 7.26079814e-03, -9.30597635e-03, -3.01832998e-04,\n",
       "         -2.01598631e-02,  5.70192653e-03, -4.61497134e-04,\n",
       "         -1.19870878e-02, -1.10290880e-02, -8.80863847e-03,\n",
       "          6.54746173e-03],\n",
       "        [ 7.27183886e-03,  1.22940119e-03,  6.48282197e-03,\n",
       "         -5.80902249e-03,  1.49400597e-02,  1.09174014e-02,\n",
       "         -3.67113311e-03, -2.01961610e-03, -2.65025974e-02,\n",
       "         -2.36367608e-03],\n",
       "        [-9.24158084e-03, -1.99155569e-02, -1.80341636e-03,\n",
       "         -1.03450720e-02,  1.02864233e-02,  1.83531099e-02,\n",
       "         -1.22676522e-02,  2.71940173e-04, -6.48377748e-03,\n",
       "         -3.41121745e-03],\n",
       "        [-1.00782048e-02, -3.97596461e-03,  2.86590418e-03,\n",
       "         -2.25645437e-03,  9.49947355e-03, -2.49815647e-03,\n",
       "          5.04357209e-03, -4.55638689e-03, -3.53313253e-03,\n",
       "          8.66822484e-04],\n",
       "        [ 7.99756938e-03, -5.90037812e-03, -1.67595910e-02,\n",
       "         -1.71222611e-02,  1.71176886e-02,  1.81963757e-02,\n",
       "         -1.44096295e-02, -5.87824564e-03,  5.15225943e-03,\n",
       "          2.43093646e-02],\n",
       "        [ 6.11581546e-03, -5.38385163e-04,  1.18616724e-02,\n",
       "          2.69116413e-04, -1.15218988e-02, -6.76009832e-03,\n",
       "         -1.97663687e-03,  1.62289188e-02,  4.73235434e-04,\n",
       "          1.35649300e-03],\n",
       "        [-1.22032969e-03,  4.80696980e-03,  1.58695073e-02,\n",
       "         -4.44326407e-03,  1.09970300e-02, -9.20747145e-03,\n",
       "          8.14374814e-03,  2.89180239e-03, -7.35892646e-04,\n",
       "          9.98882113e-03],\n",
       "        [-1.72181238e-02, -5.98915593e-03,  1.20473588e-03,\n",
       "          1.04006316e-02,  1.89315253e-03, -5.74499168e-03,\n",
       "          1.95690214e-02, -6.88492369e-03,  7.51565479e-04,\n",
       "          2.65177356e-03],\n",
       "        [ 8.78368989e-03,  5.62042587e-03,  4.79134785e-03,\n",
       "         -1.06981570e-02,  1.14836946e-03, -2.86393637e-03,\n",
       "         -4.92496103e-03,  5.44144729e-03,  1.10760317e-02,\n",
       "         -1.45992102e-02],\n",
       "        [ 8.91928066e-03, -2.70316527e-02, -1.35634269e-02,\n",
       "         -3.55231802e-03, -7.54673496e-03, -1.07369813e-02,\n",
       "          3.42206516e-03,  1.32957640e-02, -1.13985340e-02,\n",
       "          1.90830438e-02],\n",
       "        [-8.72997119e-03, -5.18446801e-03, -4.81124587e-05,\n",
       "          2.01280187e-02,  1.90233500e-03, -2.77552200e-04,\n",
       "         -1.04521151e-02, -9.88282213e-03, -1.03223977e-02,\n",
       "         -4.40287848e-03],\n",
       "        [-1.24682262e-03,  1.19746993e-02, -1.31450490e-02,\n",
       "          1.68106378e-03, -5.24744378e-03,  6.42824230e-03,\n",
       "          2.12596582e-03, -2.31322224e-03, -1.21567440e-03,\n",
       "         -1.53236658e-03],\n",
       "        [ 2.17202910e-03, -1.31389667e-02, -1.21883704e-02,\n",
       "          2.03038557e-03,  3.61384670e-04, -6.40114897e-03,\n",
       "          2.64263314e-02,  1.34625725e-02,  9.35821972e-03,\n",
       "          9.59688592e-03],\n",
       "        [-8.63840654e-03,  4.72395282e-03, -4.93661125e-04,\n",
       "         -2.15973365e-03, -2.46960848e-03,  9.13737938e-03,\n",
       "         -1.55292283e-02,  5.57439347e-04, -1.41926854e-03,\n",
       "         -6.04654997e-06],\n",
       "        [ 4.30227267e-03, -8.14674301e-03,  7.71848408e-03,\n",
       "          1.91071666e-02,  1.24514450e-02,  4.28281502e-03,\n",
       "          6.11444910e-03, -4.12768503e-03,  1.29380068e-02,\n",
       "          9.35307321e-03],\n",
       "        [ 1.74827074e-02, -6.69406580e-03, -7.91543905e-03,\n",
       "          7.85132378e-03, -4.01659256e-03, -5.96625157e-03,\n",
       "         -5.77325815e-03, -3.52014983e-03,  2.42387236e-03,\n",
       "          8.24553795e-04],\n",
       "        [ 3.09660911e-03,  8.44697242e-03, -1.78162453e-02,\n",
       "         -6.88095899e-03, -8.26978573e-03, -3.45787916e-03,\n",
       "          2.42243141e-03, -1.14464348e-02, -6.94101689e-03,\n",
       "          1.69575550e-02],\n",
       "        [ 4.26900254e-03,  1.51927549e-02,  3.48161282e-03,\n",
       "          2.49819445e-03,  9.68577057e-03,  3.99859082e-03,\n",
       "          9.88075133e-03, -1.50207266e-02,  1.20773446e-02,\n",
       "         -6.89106267e-03],\n",
       "        [-1.71414213e-02,  1.25153672e-02,  1.65385490e-02,\n",
       "          3.96471138e-04,  1.45486251e-03, -1.06501595e-02,\n",
       "          6.75294302e-03,  1.36635089e-03,  1.69981394e-02,\n",
       "          1.44241587e-02],\n",
       "        [-3.32156026e-03,  3.46576202e-03, -1.95565555e-02,\n",
       "          7.31308998e-03,  1.01216433e-02, -1.76955855e-03,\n",
       "          4.33328704e-03,  1.35337088e-02, -2.89712385e-03,\n",
       "          1.38969720e-03],\n",
       "        [-7.59192718e-03, -1.15486774e-02,  1.47838476e-02,\n",
       "         -7.06180448e-03, -1.18316963e-02,  1.26623590e-02,\n",
       "          7.11063298e-03,  5.83853849e-03,  2.11955979e-03,\n",
       "         -1.19752622e-02],\n",
       "        [-4.82157967e-03, -4.52706379e-03,  1.00180288e-02,\n",
       "          1.43793279e-02, -6.63220593e-03,  3.69569737e-03,\n",
       "          1.88278992e-02, -6.88174402e-03,  9.05308411e-03,\n",
       "         -5.33341397e-04],\n",
       "        [-2.45176218e-03, -1.18655085e-02,  1.19911878e-02,\n",
       "         -8.90958906e-03, -3.31441235e-03,  1.71006327e-02,\n",
       "         -1.81967458e-03,  1.72052463e-03, -5.70120255e-03,\n",
       "          1.33654572e-02],\n",
       "        [ 3.87735682e-03,  8.21660012e-03, -8.53744587e-03,\n",
       "          1.09584255e-02,  8.37415217e-03,  2.29784328e-03,\n",
       "          1.51668365e-02, -9.98081345e-03,  1.21124514e-02,\n",
       "          4.84765088e-03],\n",
       "        [-7.54900778e-03,  3.31099724e-04, -3.93226640e-03,\n",
       "         -5.47998685e-03,  7.04286263e-03, -2.96153208e-03,\n",
       "          1.20422594e-02, -1.33428409e-03,  1.91563878e-02,\n",
       "         -9.58335547e-03],\n",
       "        [-9.53328205e-03,  1.45262872e-02, -1.82081681e-02,\n",
       "          1.36454015e-03, -6.99785663e-03,  2.46431840e-02,\n",
       "          5.68392031e-05,  9.22484742e-03, -3.61122849e-03,\n",
       "          4.69923999e-03],\n",
       "        [-3.54430596e-03,  1.96040318e-02,  2.90619858e-03,\n",
       "         -1.92889754e-02,  8.72266970e-03, -4.84654120e-03,\n",
       "          4.66435397e-03, -2.60961881e-03, -2.45178723e-03,\n",
       "          2.71147874e-02],\n",
       "        [-1.01714235e-02,  1.82258897e-02, -6.93304064e-03,\n",
       "         -8.46193962e-03, -5.54457740e-03,  1.71767068e-02,\n",
       "          6.02296617e-03, -9.45159669e-03,  1.48528554e-02,\n",
       "          1.27057197e-02],\n",
       "        [-1.44095481e-02,  1.54533054e-02,  9.69685473e-03,\n",
       "         -3.48736383e-03,  2.37189886e-03, -3.72790735e-03,\n",
       "          4.20488373e-03,  9.54063863e-03,  6.30357991e-03,\n",
       "          7.92997196e-03],\n",
       "        [ 3.69284882e-03, -1.94869305e-03,  9.06635966e-03,\n",
       "          1.16753032e-03,  1.95359128e-03,  5.77073759e-03,\n",
       "          5.25902365e-03,  1.42460822e-02, -1.06975632e-02,\n",
       "         -1.27706235e-02],\n",
       "        [ 1.03550554e-02,  4.41965118e-03, -5.61828203e-03,\n",
       "          1.25954798e-02, -1.28664536e-02, -9.68796599e-03,\n",
       "          8.09570386e-03,  1.06171319e-02, -4.92734866e-03,\n",
       "         -1.47122790e-02],\n",
       "        [-3.45174292e-03, -8.90803695e-03, -4.95273701e-03,\n",
       "          2.01734989e-02, -7.73192819e-03, -9.93544219e-03,\n",
       "          9.35211728e-03,  5.77416249e-03,  2.62592647e-03,\n",
       "          3.33301297e-03],\n",
       "        [ 2.62556816e-02,  5.14378860e-03, -3.50385390e-03,\n",
       "         -2.44447413e-02,  1.51712980e-03, -1.71442530e-02,\n",
       "          5.17842494e-04, -9.24320785e-03,  5.39626578e-03,\n",
       "         -2.44714580e-03],\n",
       "        [ 1.33347347e-02, -3.07313435e-03, -7.31304784e-03,\n",
       "          1.54573664e-03,  1.92888092e-02, -4.75142000e-03,\n",
       "          1.29809873e-02, -2.97776270e-03,  8.15627164e-03,\n",
       "          4.59788030e-03],\n",
       "        [ 1.90647149e-02, -9.35588673e-03, -2.21278850e-03,\n",
       "         -7.19861207e-03, -1.14438193e-03,  4.84354680e-03,\n",
       "         -1.68671254e-03,  1.23202569e-02, -7.12850064e-03,\n",
       "          7.77591717e-04],\n",
       "        [-7.00008572e-03,  2.10122116e-02,  1.03736633e-02,\n",
       "         -1.05611369e-02,  6.17781632e-03, -2.27953662e-02,\n",
       "         -9.28884881e-03, -8.91153854e-03,  3.73526487e-03,\n",
       "          8.61848147e-03],\n",
       "        [-2.87577642e-04, -1.34328371e-02, -7.19956051e-03,\n",
       "         -3.76762403e-03, -5.91104897e-04, -4.17255008e-03,\n",
       "         -1.18208295e-02, -2.12761472e-03, -2.67472824e-03,\n",
       "         -1.98790973e-03],\n",
       "        [-2.52707096e-03, -4.70657145e-03, -4.70458849e-03,\n",
       "          5.70567546e-03, -7.75254098e-03, -1.77865014e-02,\n",
       "         -4.72145695e-03, -1.99647691e-03,  9.73271632e-03,\n",
       "          1.08072238e-02],\n",
       "        [ 7.83797879e-03, -3.72301018e-03,  1.85230918e-02,\n",
       "          5.39199990e-03, -3.57372204e-03,  3.63774370e-03,\n",
       "          1.40771442e-02,  2.21951165e-03,  4.21707223e-03,\n",
       "         -1.11305702e-02],\n",
       "        [ 4.75525315e-03, -7.52378764e-03,  6.26335559e-03,\n",
       "          2.59305977e-03, -5.46498436e-03, -7.50387066e-03,\n",
       "          3.54410842e-03, -1.41039590e-02,  1.47398257e-02,\n",
       "         -2.08841006e-03],\n",
       "        [ 3.95430707e-03,  5.01159785e-03,  6.85397144e-04,\n",
       "          4.07637237e-03,  4.03142690e-03, -2.10773150e-02,\n",
       "         -1.23810475e-02,  1.85733493e-03, -1.02517464e-02,\n",
       "          5.31302201e-03],\n",
       "        [-1.50602987e-02, -1.32390964e-02, -8.81313526e-03,\n",
       "         -1.86647548e-02,  1.58876724e-02,  3.08485041e-02,\n",
       "          4.66165053e-03, -1.05795345e-02,  7.30296833e-03,\n",
       "          2.75598670e-03],\n",
       "        [ 8.99054377e-03,  1.96534652e-02, -1.52755495e-02,\n",
       "          3.68857937e-03,  1.12613349e-03,  3.40862351e-03,\n",
       "          4.39217820e-03, -6.53383399e-04, -6.48685178e-03,\n",
       "          7.38488161e-03],\n",
       "        [ 1.22202744e-04,  6.56728994e-03, -3.16977483e-03,\n",
       "          1.54199412e-02, -9.37310292e-03, -4.32250612e-03,\n",
       "         -1.80440967e-03,  1.07558545e-02, -9.87987614e-03,\n",
       "          1.89898127e-02],\n",
       "        [-1.31592835e-02, -6.45610707e-03, -3.09640443e-03,\n",
       "         -9.02593205e-04,  2.24865052e-02, -1.77686565e-03,\n",
       "          7.41462959e-03, -1.18360701e-02, -1.11277267e-02,\n",
       "          1.08270147e-02],\n",
       "        [ 7.24063710e-03,  1.32446199e-03,  9.98052330e-03,\n",
       "          1.10926183e-02,  1.11823027e-03, -5.96144216e-03,\n",
       "         -5.00756374e-03,  1.70484449e-03,  9.56437783e-03,\n",
       "         -3.31455419e-03],\n",
       "        [ 2.21354238e-03, -1.47341180e-02,  9.35769312e-04,\n",
       "          6.04144973e-03, -1.25735524e-04,  1.62680037e-02,\n",
       "         -8.12542396e-03, -4.60642617e-03,  8.18297417e-03,\n",
       "         -5.30317250e-03],\n",
       "        [ 4.71172782e-03, -5.52724340e-04, -4.44257315e-03,\n",
       "          9.01778983e-03, -2.86923315e-03,  9.91355006e-03,\n",
       "         -2.10685720e-03, -1.07474821e-02,  1.07767657e-03,\n",
       "          1.42125708e-02],\n",
       "        [ 4.05283411e-03,  5.75719883e-03, -1.63878823e-02,\n",
       "         -5.48572636e-03, -1.11966762e-02,  1.27768272e-02,\n",
       "          9.84199831e-04, -4.68613942e-03,  1.95922149e-03,\n",
       "         -8.82326573e-03],\n",
       "        [-5.16048854e-03,  9.77370832e-03, -8.81399422e-03,\n",
       "          1.22634924e-02,  3.76201483e-03, -8.48138227e-03,\n",
       "          4.50315485e-04, -7.20338916e-04, -3.29364339e-04,\n",
       "         -1.14768566e-02],\n",
       "        [-6.07889705e-04,  2.97086086e-03,  8.75733013e-03,\n",
       "         -1.00897549e-02,  5.27167808e-03,  9.35233156e-03,\n",
       "         -4.19220130e-03, -5.77399672e-04,  2.46464660e-03,\n",
       "          3.60095749e-03],\n",
       "        [ 9.41454440e-03, -8.26766099e-04,  7.21762902e-03,\n",
       "         -9.11992100e-03,  4.55514035e-04, -1.16004688e-02,\n",
       "          5.51699421e-03,  5.03661065e-03,  6.61545346e-04,\n",
       "          5.53023738e-03],\n",
       "        [ 7.51294986e-04,  2.91848581e-03,  9.94471300e-03,\n",
       "         -7.95388856e-03,  6.04333789e-03,  5.03281239e-04,\n",
       "         -3.74164256e-03, -4.19805757e-05, -1.24578364e-02,\n",
       "          2.52393669e-02],\n",
       "        [ 5.00598414e-03,  2.84651597e-03,  1.79040387e-02,\n",
       "         -1.23305111e-02,  2.07931456e-03, -1.88664875e-03,\n",
       "         -7.89334484e-03,  7.42395832e-03,  9.87641481e-03,\n",
       "          6.12053517e-03],\n",
       "        [-2.77655110e-03, -1.73080420e-02,  8.99360468e-03,\n",
       "          4.24485821e-03, -3.45491502e-03, -1.80295322e-03,\n",
       "         -5.75238772e-05,  2.39016840e-03, -1.84815805e-02,\n",
       "         -2.65778765e-02],\n",
       "        [-2.61108954e-03,  6.59406122e-03, -8.06595198e-03,\n",
       "          6.89197666e-03,  7.65521035e-03, -1.03211160e-02,\n",
       "         -2.45247562e-03, -1.12841456e-02, -3.65706409e-03,\n",
       "          9.64202815e-03],\n",
       "        [ 8.71414921e-03,  3.00798975e-03,  1.00330639e-02,\n",
       "         -3.40052547e-04,  1.84228528e-02,  6.80792970e-03,\n",
       "         -2.38126736e-03,  4.41901239e-03, -2.12528611e-02,\n",
       "         -6.33608778e-03],\n",
       "        [ 1.64301589e-02,  1.12947643e-02, -1.17868835e-02,\n",
       "         -9.52661084e-03, -4.62630697e-04,  3.50126142e-03,\n",
       "          8.83047589e-03,  1.38086579e-02,  1.10768968e-03,\n",
       "          2.40147902e-03],\n",
       "        [-4.86911847e-03, -1.13044741e-02, -1.72775112e-03,\n",
       "         -1.24379520e-03, -1.16096476e-02, -1.46665273e-02,\n",
       "         -1.01468330e-02,  8.87030758e-04,  8.82092669e-03,\n",
       "          4.15573968e-03],\n",
       "        [ 1.52555396e-02, -1.81537076e-04, -5.68427057e-03,\n",
       "         -1.28755141e-02, -2.04112479e-05,  2.15418362e-02,\n",
       "         -1.46674398e-02,  1.62050273e-02,  1.18768192e-03,\n",
       "          1.18324361e-03],\n",
       "        [ 1.34781428e-02, -1.24478375e-02, -7.09470606e-04,\n",
       "          4.49040326e-03, -3.73878738e-03,  1.40029440e-02,\n",
       "          6.34708106e-04,  1.05350252e-02, -9.31940482e-03,\n",
       "          5.87946740e-04],\n",
       "        [-1.34302339e-02, -7.04214598e-04,  8.52506709e-03,\n",
       "          1.22252950e-03,  7.51558940e-03, -1.09087465e-02,\n",
       "         -2.06364108e-02,  6.24416154e-03, -2.33196186e-03,\n",
       "         -6.99320483e-03],\n",
       "        [ 1.20010917e-04, -1.42509765e-02,  2.98755298e-03,\n",
       "         -1.40034459e-02, -1.56040159e-02, -2.49629388e-03,\n",
       "         -1.12466556e-02,  1.52089752e-03,  1.03485052e-02,\n",
       "          2.05484996e-02],\n",
       "        [-4.77820475e-03,  6.06629636e-03, -1.00858818e-02,\n",
       "          2.14081931e-05, -1.23727076e-02, -6.83072185e-03,\n",
       "         -7.15824531e-03, -2.88617109e-03, -7.96524613e-03,\n",
       "          8.73467081e-03],\n",
       "        [-1.95586264e-03, -4.70081038e-03,  7.55949840e-03,\n",
       "          6.49240133e-03, -3.16717944e-03, -2.44957252e-03,\n",
       "          1.13236229e-02, -1.09182243e-02,  1.34574920e-03,\n",
       "          7.37690835e-04],\n",
       "        [ 1.76239197e-02,  8.82496439e-04,  1.67471975e-03,\n",
       "          3.67427612e-03,  5.58204302e-03,  7.86332952e-03,\n",
       "          1.08509604e-02,  1.74025435e-03, -8.35268570e-03,\n",
       "         -6.86996087e-03],\n",
       "        [ 8.84733038e-03,  1.38241666e-02,  6.50190936e-03,\n",
       "          8.02624702e-03,  1.50494378e-03,  1.69263653e-03,\n",
       "         -1.16582010e-02, -1.12949339e-02, -2.07634095e-02,\n",
       "          4.29639825e-03],\n",
       "        [ 7.76327650e-03, -1.46007710e-02,  2.12651889e-03,\n",
       "         -1.22045724e-05, -7.05054129e-04, -8.45606849e-03,\n",
       "         -4.80600831e-03,  2.18398867e-02, -5.16633724e-03,\n",
       "          9.19160898e-03],\n",
       "        [ 1.10968000e-02,  1.15710611e-02, -1.42515013e-02,\n",
       "         -7.80671118e-03,  1.52138493e-03, -3.41867565e-03,\n",
       "          2.34201677e-03,  1.25181984e-03, -8.17494790e-03,\n",
       "         -1.09682400e-02],\n",
       "        [-5.09026692e-03,  7.11152069e-03,  9.39700911e-03,\n",
       "          7.55055831e-03, -3.79045221e-03,  1.04109096e-02,\n",
       "         -2.78866664e-03, -1.83597919e-04, -4.34619530e-03,\n",
       "         -2.45026003e-03],\n",
       "        [-8.26705089e-03,  8.40665899e-04, -8.58221612e-03,\n",
       "         -2.11266609e-02, -6.70917416e-03, -4.69589045e-03,\n",
       "          3.29016401e-03, -3.42098315e-03, -4.86083127e-03,\n",
       "         -7.33428442e-03],\n",
       "        [ 2.95277150e-03,  1.10044970e-02, -1.12488547e-02,\n",
       "          1.69146470e-02,  1.38352422e-02,  1.10193748e-02,\n",
       "         -3.63040406e-04, -2.50614017e-03,  3.74596200e-03,\n",
       "          2.37268362e-03],\n",
       "        [-8.15865202e-03,  3.56309791e-03, -9.06153584e-03,\n",
       "         -2.20429088e-03, -1.78285381e-03, -1.90612478e-02,\n",
       "          1.21703089e-02, -2.35800687e-03,  6.28691424e-03,\n",
       "          6.81259745e-03],\n",
       "        [-9.24877996e-03, -1.18412842e-02,  8.24221351e-04,\n",
       "          4.50425600e-03,  7.87579739e-03,  1.98642121e-03,\n",
       "         -1.26261269e-02,  2.61288137e-02,  2.60073376e-03,\n",
       "         -1.65337600e-02],\n",
       "        [ 1.68685780e-02, -1.77784494e-02,  4.79198742e-03,\n",
       "         -5.48894296e-03, -3.32239693e-03, -7.34734723e-03,\n",
       "         -2.01922318e-03, -1.28580621e-02,  1.34678835e-02,\n",
       "          8.90039635e-04],\n",
       "        [-1.36361239e-02,  1.11534381e-02, -2.13691365e-03,\n",
       "          2.62038341e-03,  9.56219245e-03,  1.20604779e-02,\n",
       "         -9.56736657e-03,  8.61056660e-03, -1.10820363e-02,\n",
       "         -1.35296997e-02],\n",
       "        [-5.46385073e-04, -6.25843731e-03, -1.56095351e-03,\n",
       "         -3.38229917e-03, -2.41906282e-03, -2.76346685e-03,\n",
       "         -2.43174730e-03,  1.84103695e-02, -1.41013523e-02,\n",
       "         -8.01977855e-03],\n",
       "        [-1.75117593e-02, -1.60446586e-02, -1.12052572e-02,\n",
       "         -4.06906165e-03, -2.00086299e-02, -2.66282048e-02,\n",
       "          5.69774668e-03,  6.55513443e-03, -1.47779720e-02,\n",
       "          1.44855827e-02],\n",
       "        [ 1.37514251e-02,  2.25064543e-04, -2.43399751e-03,\n",
       "          1.42910986e-02, -1.15057574e-02, -1.52631455e-02,\n",
       "          1.77862957e-02,  1.64121328e-02,  5.92260881e-03,\n",
       "         -2.36573430e-02],\n",
       "        [-2.36819280e-03,  9.19022751e-03,  1.60343003e-02,\n",
       "         -3.60444652e-03, -5.46787406e-03,  1.36056354e-02,\n",
       "         -5.28754400e-03, -6.97042411e-03, -1.40267340e-03,\n",
       "          2.86958140e-03],\n",
       "        [-1.13404170e-03, -5.38141689e-03,  9.49231790e-03,\n",
       "         -1.22774916e-03, -6.27579441e-03, -3.34121820e-03,\n",
       "         -9.26217555e-03, -2.69784477e-03, -9.40639119e-05,\n",
       "         -1.19707528e-03],\n",
       "        [ 1.51299683e-02,  1.08240355e-03, -1.10055750e-03,\n",
       "          1.02829321e-02, -1.28705598e-03, -3.18167242e-03,\n",
       "         -2.62988957e-03, -9.39027435e-03,  2.00263898e-03,\n",
       "         -4.74499807e-03],\n",
       "        [-7.56474299e-03,  1.27116038e-02,  2.95395665e-02,\n",
       "         -1.84716943e-05, -5.11490808e-04, -9.88500849e-03,\n",
       "         -4.95936491e-03, -1.08894195e-02, -7.35156337e-03,\n",
       "          2.66871359e-03]]),\n",
       " 'b2': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 10 #10000\n",
    "train_size = x_train[:100].shape[0]\n",
    "batch_size = 2 #100\n",
    "lr = 0.1\n",
    "\n",
    "iter_per_ecoph = max(train_size/batch_size, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_accs = []\n",
    "test_accs = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test_acc : 0.11236666666666667, 0.1135\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m x_batch \u001b[38;5;241m=\u001b[39m x_train[batch_mask]\n\u001b[1;32m      4\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_train[batch_mask]\n\u001b[0;32m----> 6\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb2\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      9\u001b[0m     network\u001b[38;5;241m.\u001b[39mparams[key] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m lr\u001b[38;5;241m*\u001b[39mgrads[key]\n",
      "File \u001b[0;32m~/Documents/ece5831-2024-code/06/two_layer_net.py:70\u001b[0m, in \u001b[0;36mTwoLayerNet.numerical_gradient\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     67\u001b[0m loss_w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m w: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, y)\n\u001b[1;32m     69\u001b[0m grads \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 70\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_numerical_gradient(loss_w, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     72\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_numerical_gradient(loss_w, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/ece5831-2024-code/06/two_layer_net.py:54\u001b[0m, in \u001b[0;36mTwoLayerNet._numerical_gradient\u001b[0;34m(self, f, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m tmp_val \u001b[38;5;241m=\u001b[39m x[idx]\n\u001b[1;32m     53\u001b[0m x[idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(tmp_val) \u001b[38;5;241m+\u001b[39m h\n\u001b[0;32m---> 54\u001b[0m fxh1 \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# f(x+h)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m x[idx] \u001b[38;5;241m=\u001b[39m tmp_val \u001b[38;5;241m-\u001b[39m h \n\u001b[1;32m     57\u001b[0m fxh2 \u001b[38;5;241m=\u001b[39m f(x) \u001b[38;5;66;03m# f(x-h)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ece5831-2024-code/06/two_layer_net.py:67\u001b[0m, in \u001b[0;36mTwoLayerNet.numerical_gradient.<locals>.<lambda>\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumerical_gradient\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y):\n\u001b[0;32m---> 67\u001b[0m     loss_w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m w: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     grads \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     70\u001b[0m     grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_numerical_gradient(loss_w, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/ece5831-2024-code/06/two_layer_net.py:31\u001b[0m, in \u001b[0;36mTwoLayerNet.loss\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y):\n\u001b[0;32m---> 31\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mcross_entropy_error(y_hat, y)\n",
      "File \u001b[0;32m~/Documents/ece5831-2024-code/06/two_layer_net.py:23\u001b[0m, in \u001b[0;36mTwoLayerNet.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m w1, w2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw1\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     21\u001b[0m b1, b2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 23\u001b[0m a1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b1\n\u001b[1;32m     24\u001b[0m z1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivations\u001b[38;5;241m.\u001b[39msigmoid(a1)\n\u001b[1;32m     25\u001b[0m a2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(z1, w2) \u001b[38;5;241m+\u001b[39m b2\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(iterations):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "\n",
    "    grads = network.numerical_gradient(x_batch, y_batch)\n",
    "\n",
    "    for key in ('w1', 'b1', 'w2', 'b2'):\n",
    "        network.params[key] -= lr*grads[key]\n",
    "\n",
    "    ## this is for plotting losses over time\n",
    "    train_losses.append(network.loss(x_batch, y_batch))\n",
    "\n",
    "    if i%iter_per_ecoph == 0:\n",
    "        train_acc = network.accuracy(x_train, y_train)\n",
    "        train_accs.append(train_acc)\n",
    "        test_acc = network.accuracy(x_test, y_test)\n",
    "        test_accs.append(test_acc)\n",
    "        print(f'train acc, test_acc : {train_acc}, {test_acc}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_accs))\n",
    "plt.plot(x, train_accs, label='train acc')\n",
    "plt.plot(x, test_accs, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        self.activations = Activations()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.activations.sigmoid(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, w, b):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # \n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(self.x, self.w) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.w.T)\n",
    "        self.dw = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  \n",
    "        return dx\n",
    "\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None \n",
    "        self.y_hat = None    \n",
    "        self.y = None    \n",
    "        self.activations = Activations()\n",
    "        self.errors = Errors()\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        self.y = y\n",
    "        self.y_hat = self.activations.softmax(x)\n",
    "        self.loss = self.errors.cross_entropy_error(self.y_hat, self.y)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.y.shape[0]\n",
    "        #if self.y.size == self.y_hat.size: # one hot encoding\n",
    "        \n",
    "        dx = (self.y_hat - self.y) / batch_size\n",
    "        \n",
    "        \"\"\"\n",
    "        else:\n",
    "            dx = self.y_hat.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \"\"\"\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Layer Net with Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from activations import Activations\n",
    "from errors import Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (2848395530.py, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[26], line 23\u001b[0;36m\u001b[0m\n\u001b[0;31m    def predict(self, x)\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "class TwoLayerNetWithBackProp:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "\n",
    "        self.params['w1'] = weight_init_std*np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "\n",
    "        self.params['w2'] = weight_init_std*np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        self.activations = Activations()\n",
    "        self.errors = Errors()\n",
    "\n",
    "        # add layers\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['w1'], self.params['b1'])\n",
    "        self.layers['Rele1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['w2'], self.params['b2'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        ## new implementation for backprop\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        y = x\n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        y_hat = self.predict(x)\n",
    "\n",
    "        # return self.errors.cross_entropy_error(y_hat, y)\n",
    "        return self.last_layer.forward(y_hat, y)\n",
    "\n",
    "    def accuracy(self, x, y):\n",
    "        y_hat = self.predict(x)\n",
    "        p = np.argmax(y_hat, axis=1)\n",
    "        y_p = np.argmax(y, axis=1)\n",
    "\n",
    "        return np.sum(p == y_p)/float(x.shape[0])\n",
    "    \n",
    "\n",
    "    # for multi-dimensional x\n",
    "    def _numerical_gradient(self, f, x):\n",
    "        h = 1e-4 # 0.0001\n",
    "        grad = np.zeros_like(x)\n",
    "        \n",
    "        it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            idx = it.multi_index\n",
    "            tmp_val = x[idx]\n",
    "            x[idx] = float(tmp_val) + h\n",
    "            fxh1 = f(x) # f(x+h)\n",
    "            \n",
    "            x[idx] = tmp_val - h \n",
    "            fxh2 = f(x) # f(x-h)\n",
    "            grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "            \n",
    "            x[idx] = tmp_val \n",
    "            it.iternext()   \n",
    "            \n",
    "        return grad\n",
    "    \n",
    "\n",
    "    def numerical_gradient(self, x, y):\n",
    "        loss_w = lambda w: self.loss(x, y)\n",
    "\n",
    "        grads = {}\n",
    "        grads['w1'] = self._numerical_gradient(loss_w, self.params['w1'])\n",
    "        grads['b1'] = self._numerical_gradient(loss_w, self.params['b1'])\n",
    "        grads['w2'] = self._numerical_gradient(loss_w, self.params['w2'])\n",
    "        grads['b2'] = self._numerical_gradient(loss_w, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "    \n",
    "\n",
    "    def gradient(self, x, y):\n",
    "        self.loss(x, y)\n",
    "\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        grads = {}\n",
    "        grads['w1'] = self.layers['Affine1'].dw\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['w2'] = self.layers['Affine2'].dw\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "      \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece5831-2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
